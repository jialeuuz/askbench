<div align="center">

# When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification

[![è®ºæ–‡](https://img.shields.io/badge/Paper-PDF-blue?logo=adobeacrobatreader&logoColor=white)](https://arxiv.org/pdf/2602.11199v1)
[![arXiv](https://img.shields.io/badge/arXiv-2602.11199-b31b1b.svg)](https://arxiv.org/abs/2602.11199)
[![HuggingFace (Bench)](https://img.shields.io/badge/HuggingFace-askbench__bench-yellow?logo=huggingface&logoColor=black)](https://huggingface.co/datasets/jialeuuz/askbench_bench)
[![HuggingFace (Train)](https://img.shields.io/badge/HuggingFace-askbench__train-yellow?logo=huggingface&logoColor=black)](https://huggingface.co/datasets/jialeuuz/askbench_train)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

[ä¸­æ–‡](readme_zh.md) | [English](README.md) | [LLM å¯¼è¯»](readme_for_ai_zh.md)

</div>

æœ¬ä»“åº“åŒ…å«è®ºæ–‡ **â€œWhen and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarificationâ€** çš„ä»£ç ä¸ç›¸å…³èµ„æºã€‚è®ºæ–‡å·²å‘å¸ƒåœ¨ arXivï¼šğŸ”— [abs](https://arxiv.org/abs/2602.11199) | [pdf](https://arxiv.org/pdf/2602.11199v1)ã€‚

å¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹**ä¿¡æ¯ä¸è¶³**æˆ–**åŒ…å«è¯¯å¯¼å‰æ**çš„æé—®æ—¶ï¼Œå¾€å¾€ä»ä¼šç›´æ¥ä½œç­”ï¼Œä»è€Œäº§ç”Ÿå¹»è§‰æˆ–å¼ºåŒ–é”™è¯¯è®¤çŸ¥ã€‚æœ¬é¡¹ç›®ç ”ç©¶æ¨¡å‹åº”è¯¥**ä½•æ—¶**ä»¥åŠ**é—®ä»€ä¹ˆ**æ¥è¿›è¡Œæ¾„æ¸…ï¼Œå¹¶æä¾›ï¼š

- **AskBench**ï¼šä¸€ä¸ªäº¤äº’å¼åŸºå‡†ï¼Œå°†æ ‡å‡† QA æ ·æœ¬è½¬æ¢ä¸ºå¸¦æ˜¾å¼æ£€æŸ¥ç‚¹çš„å¤šè½®äº¤äº’ã€‚
- ä¸€ä¸ª**ç»Ÿä¸€çš„ Judge Loop**ï¼šåœ¨è¯„æµ‹ä¸­åŒæ—¶å®Œæˆ (1) æœ€ç»ˆç­”æ¡ˆè¯„ä¼°ï¼Œä»¥åŠ (2) å½“è¢«æµ‹æ¨¡å‹å‘èµ·è¿½é—®æ—¶æ¨¡æ‹Ÿç”¨æˆ·å›å¤ã€‚
- ä¸¤ä¸ªæ ¸å¿ƒè®¾ç½®ï¼š
  - **AskMind**ï¼šæ„å›¾ç¼ºå¤±/ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ï¼Œéœ€è¦é€šè¿‡è¿½é—®è·å–å…³é”®ä¿¡æ¯åå†å›ç­”ã€‚
  - **AskOverconfidence**ï¼šé—®é¢˜åŒ…å«é”™è¯¯å‰æ/è¯¯å¯¼æ–­è¨€ï¼Œéœ€è¦è¯†åˆ«å¹¶çº æ­£åå†å›ç­”ã€‚

å¦‚æœä½ å¸Œæœ›å€ŸåŠ© LLM å¿«é€Ÿç†è§£/ä¿®æ”¹ä»£ç ç»“æ„ï¼ˆä¾¿äºè°ƒè¯•ä¸å®šä½å…¥å£ï¼‰ï¼Œå¯å…ˆé˜…è¯» `readme_for_ai.md`ï¼ˆä¸­æ–‡ç‰ˆï¼š`readme_for_ai_zh.md`ï¼‰ã€‚

## ğŸ“Œ ç›®å½•

- ğŸš€ è¯„æµ‹ï¼š [è¿è¡Œè¯„æµ‹](#evaluation)
- ğŸ¯ è®­ç»ƒï¼š [RLVR reward + VERL æ¥å…¥](#training)
- ğŸ§ª data pipelineï¼š [æ„å»º AskBench é£æ ¼æ•°æ®](#data-pipeline)
- ğŸ› ï¸ å·¥å…·ï¼š [checkpoint åˆå¹¶ + OpenAI-compatible éƒ¨ç½²](#tools)
- ğŸ“¦ æ•°æ®é›†ï¼š [Hugging Face é“¾æ¥](#datasets)

## âœ¨ AskBench é€Ÿè§ˆ

AskBench å°†â€œæ¾„æ¸…â€ä½œä¸ºä¸€ç§**äº¤äº’èƒ½åŠ›**æ¥è¯„æµ‹ã€‚æ¯ä¸ªæ ·æœ¬è¿è¡Œæ—¶åŒ…å«ï¼š

- **è¢«æµ‹æ¨¡å‹**ï¼ˆassistantï¼‰ï¼Œä»¥åŠ
- **Judge æ¨¡å‹**ï¼ˆåœ¨å¤šè½®è¯„æµ‹ä¸­æ‰¿æ‹…å¤šä¸ªè§’è‰²ï¼‰ï¼š
  - **æ¨¡æ‹Ÿç”¨æˆ·**ï¼ˆå½“ assistant è¿½é—®æ—¶è¡¥å……ä¿¡æ¯ï¼‰ï¼Œä»¥åŠ
  - **è¯„åˆ†å™¨**ï¼ˆåˆ¤æ–­æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€æ£€æŸ¥ç‚¹æ˜¯å¦è¦†ç›–å……åˆ†ï¼‰ã€‚

æ•´ä½“æµç¨‹æ˜¯ï¼šè¢«æµ‹æ¨¡å‹å¯ææ¾„æ¸…é—®é¢˜ â†’ Judge è§†æƒ…å†µæ¨¡æ‹Ÿç”¨æˆ·å›å¤ â†’ äº§å‡ºæœ€ç»ˆç­”æ¡ˆ â†’ Judge ç»™å‡ºåˆ¤å®šä¸ç»Ÿè®¡ã€‚

## ğŸ” ä¸ºä»€ä¹ˆæ˜¯ AskBenchï¼Ÿ

åœ¨çœŸå®äº¤äº’ä¸­ï¼Œç”¨æˆ·é—®é¢˜å¸¸å¸¸ **ä¿¡æ¯ä¸è¶³** æˆ–åŒ…å« **è¯¯å¯¼å‰æ**ã€‚ä¼ ç»Ÿå•è½® benchmark æ›´æ“…é•¿è¡¡é‡â€œç­”å¾—å¯¹ä¸å¯¹â€ï¼Œä½†å¾ˆéš¾è¡¡é‡ï¼š

- æ¨¡å‹æ˜¯å¦èƒ½åœ¨åˆé€‚æ—¶æœºé€‰æ‹©è¿½é—®ï¼›ä»¥åŠ
- è¿½é—®æ˜¯å¦å‘½ä¸­çœŸæ­£å…³é”®çš„ç¼ºå¤±ç‚¹/è¯¯å¯¼ç‚¹ã€‚

AskBench çš„è®¾è®¡æ—¨åœ¨è®©â€œæ¾„æ¸…èƒ½åŠ›â€å¯è§„æ¨¡åŒ–è¯„æµ‹ï¼š

- **äº¤äº’å¼ä¸”å¯è‡ªåŠ¨åŒ–**ï¼šjudge loop åœ¨æ¨¡å‹æ˜ç¡®è¿½é—®æ—¶æ‰æ¨¡æ‹Ÿç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼Œå¹¶ç«¯åˆ°ç«¯è¯„åˆ†æœ€ç»ˆç­”æ¡ˆã€‚
- **ç»†ç²’åº¦ä¸”å¯è§£é‡Š**ï¼šcheckpoints/rubrics æŠŠæ¾„æ¸…è¡Œä¸ºæ‹†æˆå¯åˆ†æçš„æ¡ç›®æŒ‡æ ‡ï¼ˆä¾‹å¦‚ checkpoint coverageï¼‰ã€‚
- **é«˜æ‹“å±•æ€§**ï¼šä¸ºæ ‡å‡† QA ç”Ÿæˆâ€œå˜ä½“é—®é¢˜â€ï¼ˆdegraded æˆ–æ³¨å…¥è¯¯å¯¼å‰æï¼‰å¹¶é…å¥— checklistï¼Œå³å¯å¿«é€Ÿæ”¹é€ ä¸ºäº¤äº’å¼è¯„æµ‹ã€‚
- **æ˜“ç”¨æ€§å¼º**ï¼šè¯„æµ‹åªä¾èµ– OpenAI-compatible APIï¼ˆè¢«æµ‹æ¨¡å‹ + judgeï¼‰ï¼Œå¯é€šè¿‡ vLLM ç­‰å·¥å…·æœ¬åœ°éƒ¨ç½²ã€‚

## ğŸ“ˆ è®ºæ–‡ç»“æœ

è®ºæ–‡ä¸­ï¼Œrubric-guided RLVR åœ¨ AskBench å¤šè½®è¯„æµ‹ä¸Šæ˜¾è‘—æå‡æ¾„æ¸…èƒ½åŠ›ï¼ŒåŒæ—¶èƒ½ä¿æŒï¼ˆç”šè‡³æå‡ï¼‰å•è½® QA ç­‰é€šç”¨èƒ½åŠ›ã€‚

### AskBench å¤šè½®æ¾„æ¸…ï¼ˆæ™®é€šåè®®ï¼ŒTable 4ï¼‰

æŒ‡æ ‡è¯´æ˜ï¼š

- `acc`ï¼ˆaccuracyï¼‰ï¼šæœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆç”± judge è¯„åˆ†ï¼‰ã€‚
- `cov`ï¼ˆcheckpoint coverageï¼‰ï¼šä½œç­”å‰å¯¹ checklist çš„è¦†ç›–ç¨‹åº¦ï¼ˆAskMind çš„ `required_points`ï¼›AskOverconfidence çš„ `misleading_points`ï¼‰ã€‚

| æ¨¡å‹ | AskMind `acc` | AskMind `cov` | AskOverconfidence `acc` | AskOverconfidence `cov` |
| --- | :---: | :---: | :---: | :---: |
| Gemini-2.5-Pro | 0.567 | 0.124 | 0.840 | 0.749 |
| GPT-4.1 | 0.495 | 0.118 | 0.730 | 0.602 |
| Qwen2.5-7B-Instruct | 0.332 | 0.214 | 0.443 | 0.188 |
| OursI | 0.615 | 0.679 | 0.628 | 0.641 |
| OursO | 0.617 | 0.807 | 0.548 | 0.894 |

### ä¸¥æ ¼ä¸¤è½®åè®®ï¼ˆâ€œHardâ€, Table 5ï¼‰

ä¸¥æ ¼ä¸¤è½®åè®®è¦æ±‚ï¼šç¬¬ 1 è½®å¿…é¡»æ¾„æ¸…/çº åï¼Œç¬¬ 2 è½®å¿…é¡»ç›´æ¥ç»™æœ€ç»ˆç­”æ¡ˆï¼ˆä¸èƒ½å†è¿½é—®ï¼‰ã€‚

| æ¨¡å‹ | AskMind `acc` | AskMind `cov` | AskOverconfidence `acc` | AskOverconfidence `cov` |
| --- | :---: | :---: | :---: | :---: |
| Gemini-2.5-Pro | 0.0551 | 0.2206 | 0.0100 | 0.7350 |
| GPT-4.1 | 0.0352 | 0.2035 | 0.0000 | 0.5865 |
| Qwen2.5-7B-Instruct | 0.0176 | 0.1288 | 0.0050 | 0.1955 |
| OursI | 0.2714 | 0.5013 | 0.1975 | 0.5065 |
| OursO | 0.1965 | 0.4235 | 0.2600 | 0.7778 |

æ³¨ï¼šè®ºæ–‡ä¸­ Gemini æŒ‡ Gemini-2.5-Proï¼ŒGPT æŒ‡ GPT-4.1ï¼ŒQwen æŒ‡ Qwen2.5-7B-Instructã€‚
OursI/OursO åˆ†åˆ«æ˜¯æˆ‘ä»¬åœ¨ AskMind/AskOverconfidence ç»´åº¦ä¸Šè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ã€‚

å•è½®å‡†ç¡®ç‡ä¸ HealthBench å¾—åˆ†ï¼ˆTable 3ï¼‰ï¼š

| æ¨¡å‹ | Math500 | MedQA | HealthBench | GPQA-d | BBH |
| --- | ---: | ---: | ---: | ---: | ---: |
| Gemini-2.5-Pro | 0.952 | 0.943 | 0.649 | 0.864 | 0.946 |
| GPT-4.1 | 0.936 | 0.918 | 0.645 | 0.701 | 0.708 |
| Qwen2.5-7B-Instruct | 0.760 | 0.653 | 0.526 | 0.309 | 0.506 |
| OursI | 0.780 | 0.936 | 0.606 | 0.497 | 0.758 |
| OursO | 0.720 | 0.992 | 0.559 | 0.781 | 0.760 |

è¯´æ˜ï¼šéƒ¨åˆ†è¯„æµ‹ï¼ˆä¾‹å¦‚ HealthBenchï¼‰å±äº LLM-judge è¯„æµ‹ã€‚ä¸ºèŠ‚çœæˆæœ¬å¹¶ä¾¿äºå¤ç°ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼€æº judgeï¼ˆå¦‚è®ºæ–‡ä¸­ä½¿ç”¨çš„ Qwen3-30B-A3B-Instruct-2507ï¼‰è€Œéé—­æº GPT judgeï¼Œå› æ­¤ç»å¯¹åˆ†æ•°å¯èƒ½ä¸å®˜æ–¹æŠ¥å‘Šå­˜åœ¨å·®å¼‚ï¼Œä½†ä¸åŒæ¨¡å‹çš„æ•´ä½“æ’åè¶‹åŠ¿é€šå¸¸ä¿æŒä¸€è‡´ã€‚

## ğŸ§© ä»“åº“ç»“æ„

- `ask_eval/`ï¼šè¯„æµ‹ pipelineï¼ˆå•è½® + AskBench é£æ ¼çš„å¤šè½®è¯„æµ‹ï¼‰ã€‚
  - ä½¿ç”¨è¯´æ˜ï¼š`ask_eval/README.md`
  - å®ç°ç»†èŠ‚/è°ƒè¯•å®šä½ï¼š`ask_eval/readme_for_ai.md`
  - å…¥å£è„šæœ¬ï¼š`ask_eval/run.sh`
- `data_pipeline/`ï¼šæ•°æ®æ„å»º pipelineï¼Œç”¨äºç”Ÿæˆ AskBench é£æ ¼çš„å¤šè½®å¯¹è¯æ•°æ®ï¼ˆ**è®­ç»ƒ + è¯„æµ‹**ï¼‰ï¼Œä»¥åŠæŠŠå¸¸è§„ QA bench å¿«é€Ÿæ”¹é€ ä¸º AskMind/AskOverconfidence é£æ ¼çš„å˜ä½“ä¸ rubric/checklistã€‚
  - ä½¿ç”¨è¯´æ˜ï¼š`data_pipeline/README.md`
  - å®ç°ç»†èŠ‚/è°ƒè¯•å®šä½ï¼š`data_pipeline/readme_for_ai.md`
  - å…¥å£è„šæœ¬ï¼š`data_pipeline/main.py`
- `reward/`ï¼šrubric-guided reward / è®­ç»ƒè¾…åŠ©è„šæœ¬ï¼ˆç”¨äº RLVR é£æ ¼è®­ç»ƒï¼‰ã€‚
- `tools/`ï¼šè¾…åŠ©è„šæœ¬ï¼Œç”¨äºï¼ˆ1ï¼‰å°†è®­ç»ƒ checkpoint è½¬æˆå¯æ¨ç†çš„ HuggingFace æ¨¡å‹ç›®å½•ï¼Œä»¥åŠï¼ˆ2ï¼‰ç”¨ vLLM éƒ¨ç½² OpenAI-compatible APIã€‚
- `readme_for_ai.md`ï¼šé¢å‘ LLM çš„ä»“åº“å¯¼è¯»ï¼ˆæ¶æ„æ¢³ç† + å…³é”®å…¥å£ï¼‰ã€‚
- `paper.pdf`ï¼šè®ºæ–‡ PDFï¼ˆåŒ¿åæŠ•ç¨¿ç‰ˆæœ¬æ„å»ºäº§ç‰©ï¼›ä»¥ arXiv ç‰ˆæœ¬ä¸ºå‡†ï¼‰ã€‚

åŸä¸­æ–‡æ–‡æ¡£å·²ç”¨ `_zh` åç¼€ä¿ç•™ï¼ˆä¾‹å¦‚ `ask_eval/README_zh.md`ï¼‰ã€‚

## âš™ï¸ ç¯å¢ƒä¸å®‰è£…

å»ºè®®ï¼šPython 3.10+ï¼Œå¹¶ä½¿ç”¨ conda ç¯å¢ƒã€‚

### å®‰è£… `ask_eval`

```bash
conda create -n askq python=3.10 -y
conda activate askq

pip install -e ./ask_eval
```

### å®‰è£… `data_pipeline` ä¾èµ–

```bash
pip install -r data_pipeline/requirements.txt
```

<a id="evaluation"></a>
## ğŸš€ Quickstartï¼šè¿è¡Œè¯„æµ‹ï¼ˆAskBench + æ ‡å‡† QAï¼‰

`ask_eval` å‡è®¾ä½ æœ‰ä¸€ä¸ª **OpenAI-compatible** çš„ chat-completions APIï¼Œåˆ†åˆ«ç”¨äºï¼š

- **è¢«æµ‹æ¨¡å‹**ï¼ˆcandidateï¼‰ï¼Œä»¥åŠ
- **Judge æ¨¡å‹**ï¼ˆè´Ÿè´£è¯„åˆ†ï¼›åœ¨ AskBench ä¸­è¿˜è´Ÿè´£æ¨¡æ‹Ÿç”¨æˆ·ï¼‰ã€‚

1) åœ¨ `ask_eval/config/base.ini`ï¼ˆä»¥åŠå¯é€‰çš„ `ask_eval/config/common/` ä»»åŠ¡çº§è¦†ç›–ï¼‰ä¸­é…ç½®æ¨¡å‹ endpoint ä¸ tokenã€‚
2) è¿è¡Œï¼š

```bash
cd ask_eval
python scripts/main.py --config config/base.ini
```

å¦‚æœå¸Œæœ›é€šè¿‡ shell å˜é‡è¦†ç›–é…ç½®é¡¹ï¼Œå¯ä½¿ç”¨ `ask_eval/run.sh`ã€‚

è¯´æ˜ï¼š

- AskBench é£æ ¼ä»»åŠ¡é€šè¿‡ `ask_eval/scripts/run_ask.py` è·‘ judge-driven å¤šè½®è¯„æµ‹ã€‚
- å¯åœ¨ `ask_eval/run.sh` ä¸­è®¾ç½® `STRICT_MODE=1` æ¥å¯ç”¨æ›´ä¸¥æ ¼çš„ä¸¤è½®åè®®ï¼ˆç¬¬ä¸€è½®å¿…é¡»æ¾„æ¸…/çº æ­£ï¼Œç¬¬äºŒè½®å¿…é¡»ç›´æ¥ç»™æœ€ç»ˆç­”æ¡ˆä¸”ä¸èƒ½å†è¿½é—®ï¼‰ã€‚
- è¯„æµ‹è¾“å‡ºå†™å…¥ `ask_eval/results/<task>/<task_name>/`ï¼Œå¹¶åœ¨ `ask_eval/results/final_result.txt` è¿½åŠ èšåˆæ±‡æ€»è¡Œã€‚

<a id="tools"></a>
## ğŸ› ï¸ å·¥å…·ï¼šcheckpoint è½¬æ¢ + OpenAI-compatible API éƒ¨ç½²

`ask_eval` é€šè¿‡ OpenAI-compatible çš„ chat-completions API è°ƒç”¨æ¨¡å‹ã€‚å¦‚æœä½ çš„å·¥ä½œæµæ˜¯åŸºäº API è°ƒç”¨ï¼Œè¿™é‡Œæä¾›äº† `tools/` ä¸‹ä¸¤ä¸ªå¸¸ç”¨è„šæœ¬ï¼Œå¯¹åº”ä¸€ä¸ªå¸¸è§æµç¨‹ï¼š

1) ï¼ˆå¯é€‰ï¼‰**æŠŠè®­ç»ƒ checkpoint è½¬æˆæ¨ç†å¯ç”¨çš„ HuggingFace æ¨¡å‹ç›®å½•**ï¼š`tools/merge.sh`ã€‚
2) **ç”¨ vLLM éƒ¨ç½²æˆ OpenAI-compatible API**ï¼š`tools/vllm.sh`ã€‚

### è®­ç»ƒ checkpoint è½¬æ¢ï¼ˆ`tools/merge.sh`ï¼‰

éƒ¨åˆ†è®­ç»ƒäº§ç‰©ï¼ˆä¾‹å¦‚ VERL/RLVR è®­ç»ƒè¾“å‡ºçš„åˆ†ç‰‡ checkpointï¼‰æ— æ³•ç›´æ¥è¢« vLLM åŠ è½½æ¨ç†ï¼Œéœ€è¦å…ˆåˆå¹¶/å¯¼å‡ºæˆæ ‡å‡† HuggingFace æ¨¡å‹æ–‡ä»¶å¤¹ã€‚

1) ä¿®æ”¹ `tools/merge.sh` ä¸­çš„å˜é‡ï¼š
   - `CHECKPOINT_DIR`ï¼šè®­ç»ƒ checkpoint è·¯å¾„ï¼ˆé€šå¸¸æ˜¯æŸä¸ª `.../actor` ç›®å½•ï¼‰
   - `OUTPUT_DIR`ï¼šå¯¼å‡ºåçš„æ¨¡å‹ç›®å½•
   - `WORLD_SIZE`ï¼šcheckpoint åˆ†ç‰‡æ•°é‡ï¼ˆä¸€èˆ¬ç­‰äºè®­ç»ƒçš„ world sizeï¼‰
   - `MERGE_SCRIPT_PATH`ï¼šä½ ç¯å¢ƒä¸­ `merge_verl.py` è½¬æ¢è„šæœ¬çš„è·¯å¾„
2) è¿è¡Œï¼š

```bash
bash tools/merge.sh
```

æˆåŠŸåï¼Œå°† `tools/vllm.sh` çš„ `MODEL_PATH` æŒ‡å‘å¯¼å‡ºçš„ `OUTPUT_DIR`ã€‚

### ç”¨ vLLM éƒ¨ç½² OpenAI-compatible APIï¼ˆ`tools/vllm.sh`ï¼‰

è¯¥è„šæœ¬å¯åŠ¨ vLLM çš„ OpenAI-compatible serverï¼ˆ`vllm.entrypoints.openai.api_server`ï¼‰ã€‚

1) ä¿®æ”¹ `tools/vllm.sh` ä¸­çš„å˜é‡ï¼š
   - `MODEL_PATH`ï¼šHuggingFace æ¨¡å‹ç›®å½•ï¼ˆå¯ä»¥æ˜¯ base æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ˜¯ `tools/merge.sh` äº§å‡ºçš„ `OUTPUT_DIR`ï¼‰
   - `CUDA_DEVICES` ä¸ `TP`ï¼šåº”ä¸å‚ä¸ tensor-parallel çš„ GPU æ•°é‡ä¸€è‡´
   - `PORT`ï¼šæœåŠ¡ç«¯å£
2) è¿è¡Œï¼š

```bash
bash tools/vllm.sh
```

ç„¶ååœ¨ `ask_eval/config/base.ini`ï¼ˆæˆ– `ask_eval/run.sh`ï¼‰ä¸­é…ç½®æœåŠ¡åœ°å€ï¼Œä¾‹å¦‚ï¼š

- `[model] api_url = http://<host>:<port>/v1`
- `[model] model_name = default`ï¼ˆéœ€ä¸ `tools/vllm.sh` ä¸­çš„ `--served-model-name` ä¸€è‡´ï¼‰

<a id="datasets"></a>
## ğŸ“¦ æ•°æ®é›†

- **Hugging Faceï¼ˆæ¨èä¸‹è½½é“¾æ¥ï¼‰**ï¼š
  - ğŸ¤— AskBench è¯„æµ‹æ•°æ®ï¼š[jialeuuz/askbench_bench](https://huggingface.co/datasets/jialeuuz/askbench_bench)
  - ğŸ¤— AskMind/AskOverconfidence è®­ç»ƒè½¨è¿¹ï¼š[jialeuuz/askbench_train](https://huggingface.co/datasets/jialeuuz/askbench_train)
- **è¯„æµ‹æ•°æ®ï¼ˆä»“åº“è·Ÿè¸ªï¼‰**ï¼šä½äº `ask_eval/data/`ï¼ˆAskBench å­é›† + pipeline ä½¿ç”¨çš„å¸¸è§„ benchmarkï¼‰ã€‚
- **å¯é€‰è®­ç»ƒ/ä¸­é—´æ•°æ®ï¼ˆä¸è·Ÿè¸ªï¼‰**ï¼šå¯æ”¾åœ¨æ ¹ç›®å½•çš„ `data/` ä¸‹ï¼ˆæœ¬ä»“åº“é»˜è®¤ `.gitignore` å¿½ç•¥ `data/`ï¼‰ã€‚

## ğŸ“ è¾“å‡ºï¼ˆä¼šå†™å“ªäº›æ–‡ä»¶ï¼‰

æ ¹æ®ä»»åŠ¡ç±»å‹ï¼Œ`ask_eval` ä¼šå†™å…¥ä»¥ä¸‹æ–‡ä»¶çš„ç»„åˆï¼š

- `results.txt`ï¼šäººç±»å¯è¯»çš„æ±‡æ€»ï¼ˆæŒ‡æ ‡ + è€—æ—¶ï¼‰ã€‚
- `summary_results.json`ï¼šå•è½®ä»»åŠ¡çš„é€æ ·æœ¬è¾“å‡ºã€‚
- `askbench_detailed_results.json`ï¼šAskBench é£æ ¼ä»»åŠ¡çš„é€è½®å¯¹è¯è½¨è¿¹ä¸ judge åˆ¤å®šç»†èŠ‚ã€‚

## ğŸ§± ç”Ÿæˆï¼ˆæˆ–é‡å»ºï¼‰AskBench åˆå¹¶è¯„æµ‹é›†

AskBench çš„ä¸»ä»»åŠ¡é€šå¸¸æ˜¯ç”±å¤šä¸ªå­é›†æ‹¼æˆçš„å°è§„æ¨¡ mixtureï¼ˆä¾‹å¦‚æ¯ä¸ªæ¥æº benchmark é‡‡æ · 100 æ¡ï¼‰ã€‚

```bash
python ask_eval/data/ask_bench/ask_mind/build_combined_eval.py
python ask_eval/data/ask_bench/ask_overconfidence/build_combined_eval.py
```

<a id="data-pipeline"></a>
## ğŸ§ª Quickstartï¼šæ„å»º AskBench é£æ ¼æ•°æ®ï¼ˆè®­ç»ƒ + è¯„æµ‹ï¼‰

æ•°æ®æ„å»º pipeline å¯ä»¥ç”Ÿæˆ AskBench é£æ ¼çš„å¤šè½®å¯¹è¯ï¼ˆæ¾„æ¸… â†’ æ¨¡æ‹Ÿç”¨æˆ·å›å¤ â†’ ä½œç­” â†’ è¯„å®¡ï¼‰ç”¨äºè®­ç»ƒï¼›åŒæ—¶ä¹Ÿå¯ä»¥æŠŠå…¶ä»– QA bench å¿«é€Ÿæ”¹é€ ä¸º AskMind/AskOverconfidence é£æ ¼çš„è¯„æµ‹æ•°æ®ï¼ˆç”Ÿæˆå˜ä½“é—®é¢˜ + checklist/rubricsï¼‰ã€‚

å…·ä½“å…¥å£ä¸å‚æ•°è¯´æ˜è§ `data_pipeline/README.md`ã€‚

<a id="training"></a>
## ğŸ¯ Rubric-guided rewardï¼ˆRLVRï¼‰

`reward/` ç›®å½•åŒ…å«ä¸¤ä¸ª **VERL å¯ç›´æ¥ä½¿ç”¨** çš„ reward å‡½æ•°å®ç°ï¼Œå¯¹åº”è®ºæ–‡ä¸­çš„ rubric-guidedã€turn-level shapingï¼š

- AskMindï¼ˆæ„å›¾ç¼ºå¤± / ä¿¡æ¯ä¸è¶³ï¼‰ï¼š`reward/ask_mind_qa.py`ï¼ˆ`data_source = ask_mind_qa`ï¼‰
- AskOverconfidenceï¼ˆè¿‡åº¦è‡ªä¿¡ / é”™è¯¯å‰æï¼‰ï¼š`reward/overconfidence_qa.py`ï¼ˆ`data_source = overconfidence_qa`ï¼‰

ä½¿ç”¨æ–¹å¼æ˜¯å°†è„šæœ¬æ‹·è´åˆ° VERL çš„ `verl/utils/reward_score/` å¹¶åœ¨ `default_compute_score()` é‡Œæ³¨å†Œï¼›judge ç«¯ç‚¹é€šè¿‡ `API_URLS` / `JUDGE_MODEL_NAME` é…ç½®ã€‚æ›´è¯¦ç»†çš„æ¥å…¥æ­¥éª¤è§ `reward/readme`ï¼Œä»£ç ç»†èŠ‚è¯´æ˜è§ `reward/readme_for_ai_zh.md`ã€‚

å¦å¤–æä¾›äº†å·²è„±æ•çš„è®­ç»ƒå¯åŠ¨è„šæœ¬å‚è€ƒï¼ˆVERL + Ray + DAPO/GRPOï¼‰ï¼š`reward/train.sh`ã€‚

## ğŸ“š å¼•ç”¨

å¦‚æœä½ ä½¿ç”¨äº†æœ¬ä»“åº“ï¼Œè¯·å¼•ç”¨è®ºæ–‡ï¼š

```bibtex
@misc{askbench2026,
  title        = {When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification},
  author       = {Anonymous},
  year         = {2026},
  note         = {Anonymous ACL submission},
}
```
