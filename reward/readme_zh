# Rubric-guided reward（RLVR）

本目录包含用于 **rubric-guided RLVR** 训练的奖励函数参考实现（论文背景见 `paper.pdf`）。

这里提供两个与论文中 AskBench 两个维度对应的 reward 模块：

- **AskMind（意图缺失 / 信息不足）**：`reward/ask_mind_qa.py`（`data_source = "ask_mind_qa"`）
- **AskOverconfidence（过度自信 / 错误前提）**：`reward/overconfidence_qa.py`（`data_source = "overconfidence_qa"`）

这两个脚本的设计目标是：直接放到 **VERL** 项目里使用（本仓库不包含 VERL 代码）：

- VERL 仓库：<https://github.com/verl-project/verl.git>
- 目标目录（在 VERL 仓库内）：`verl/utils/reward_score/`

## 1）配置 Judge 模型 API

两个 reward 文件都会通过 OpenAI-compatible 的 Chat Completions API 调用一个 **Judge 模型**。

训练前请修改每个文件开头的：

- `API_URLS`：judge 服务端点（默认给的是本地 `127.0.0.1` 的 `8012-8015` 端口）
- `JUDGE_MODEL_NAME`：需要与 vLLM 启动参数 `--served-model-name` 一致（默认 `default`）

论文设置里使用的 judge 模型是 **Qwen3-30B-A3B-Instruct-2507**（以 OpenAI-compatible API 形式部署）。

## 2）集成到 VERL

1）下载 VERL，并找到 reward 目录：

```bash
git clone https://github.com/verl-project/verl.git
cd verl
ls verl/utils/reward_score
```

2）把 reward 文件拷贝到 VERL：

```bash
cp /path/to/askQ/reward/ask_mind_qa.py verl/utils/reward_score/ask_mind_qa.py
cp /path/to/askQ/reward/overconfidence_qa.py verl/utils/reward_score/overconfidence_qa.py
```

3）修改 `verl/utils/reward_score/__init__.py`，在 `def default_compute_score(...):` 里增加：

```python
    if data_source == "ask_mind_qa":
        from .ask_mind_qa import compute_score_ask_mind_qa
        return compute_score_ask_mind_qa(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=extra_info,
        )

    if data_source == "overconfidence_qa":
        from .overconfidence_qa import compute_score_overconfidence_qa
        return compute_score_overconfidence_qa(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=extra_info,
        )
```

## 3）`extra_info` 的关键字段（概览）

两个模块都实现了 **turn-level** 的 reward：

- 非最终轮：基于 checklist 覆盖率的离散 shaping（与论文一致：`{-2.0, -0.8, 0.8, 1.0}`）。
- 最终轮：只判 `still_asking / wrong / correct`。

AskMind 模块（`ask_mind_qa.py`）常用字段：

- `is_final_turn`, `ori_question`, `degraded_info`, `required_points`, `question`, `context`, `expected_answer`

AskOverconfidence 模块（`overconfidence_qa.py`）常用字段：

- `is_final_turn`, `ori_question`, `overconfidence_info`, `misleading_points`, `question`, `context`, `expected_answer`

更细的代码结构、prompt 与 JSON schema 说明见 `reward/readme_for_ai_zh.md`。

## 4）训练脚本参考

`reward/train.sh` 是一个**已脱敏**的训练启动脚本参考（基于论文中的训练设置：VERL + Ray + `recipe.dapo.main_dapo`）。

它不会开箱即用，使用前需要你自行修改/导出：

- `VERL_DIR`：本地 VERL 仓库根目录
- `MODEL_PATH`：HuggingFace 模型目录
- `DATA_TRAIN_PATH`, `DATA_VAL_PATH`：训练/验证 parquet 数据
- `OUTPUT_DIR`：输出目录（log/checkpoint）

脚本里也会启动本地 Ray head（端口可用 `RAY_PORT` / `RAY_DASHBOARD_PORT` 配置）。
