`reward/yitu.py` implements the reward function for the **AskMind / missing-information** setting.

## Integration (VERL)

1) Copy `reward/yitu.py` into `/verl/verl/utils/reward_score/`.
2) Edit `/verl/verl/utils/reward_score/__init__.py` and add the following branch inside `def default_compute_score()`:

```python
if data_source == "medical_qa":
    from .medical_qa import compute_score_medical_qa
    return compute_score_medical_qa(
        data_source=data_source,
        solution_str=solution_str,
        ground_truth=ground_truth,
        extra_info=extra_info,
    )
```

## Configuration

- Before use, set `API_URLS` at the top of `yitu.py`. The setup in this repo uses an A3B judge model (Qwen3-30B-A3B-Instruct-2507).

## Data preparation

- `tools/process_data_for_yitu.py` converts `data/17k_dapo_7b_reject_ask.jsonl` into the training format. Configure parameters under `if __name__ == "__main__":`.

## Training

- `reward/train.sh` contains one set of training arguments. You will likely need to adjust them for your hardware (e.g., batch size). For the missing-information dimension, you may not need a 2k context window and an 8k response; reduce them if you run out of VRAM.

## Monitoring

1) A custom monitor (Gradio): configure parameters under `if __name__ == "__main__":` and run it.
2) TensorBoard: logs are under `tensorboard_log/verl_zjl` (often rooted at `/verl/tensorboard_log`). Example:

```bash
tensorboard --logdir /lpai/volumes/base-mindgpt-ali-sh-mix/zhouyang/verl_run/tensorboard_log
```

## Output comparison

- `tools/gradio_compare.py` compares outputs from two models. Search for `api1_input` to configure APIs, or configure them directly in the Gradio UI.

## Expected data schema (notes)

- `ori_question`: original question
- `expected_answer`: reference answer
- `pass_rate`: number of passes (0.0–16.0; 16.0 means 16/16 passes; stored as float but conceptually integer)
- Note: some datasets may contain extra columns (e.g., `degrade_info`). It is recommended not to rely on them if you are unsure whether they were produced by the latest `data_pipeline`.

## Misc notes

- The newer `yitu` setup may be harder to fit (lower scores). This may be because the data is math-heavy; consider trying other data types.
- The “gulang” ("lone-wolf") dimension may also benefit from trying other data types.

References / pointers:

- `open-r1/DAPO-Math-17k-Processed` (dataset)
- `RAR-med`, `RAR-science`
- “Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains”

Networking:

- `hostname -i` to get the server IP.

Data generation model used in one setup:

- `qwen3-235b-a22b-instruct-2507`
