# Rubric-guided reward (RLVR)

This directory contains reference reward functions used for **rubric-guided RLVR** training (as described in `paper.pdf`).

It provides two reward modules aligned with the paper’s two AskBench dimensions:

- **AskMind (intent-deficient / missing information)**: `reward/ask_mind_qa.py` (`data_source = "ask_mind_qa"`)
- **AskOverconfidence (misleading premises / unjustified certainty)**: `reward/overconfidence_qa.py` (`data_source = "overconfidence_qa"`)

These scripts are designed to be dropped into the **VERL** project (not vendored in this repo):

- VERL repo: `https://github.com/verl-project/verl.git`
- Target directory (inside the VERL repo): `verl/utils/reward_score/`

## 1) Configure the judge model API

Both reward modules call a **judge model** via an OpenAI-compatible Chat Completions endpoint.

Before training, edit the top of each file and set:

- `API_URLS`: your judge server endpoints (default is local `127.0.0.1` on ports `8012-8015`)
- `JUDGE_MODEL_NAME`: must match vLLM’s `--served-model-name` (default is `default`)

In our paper setup the judge model is **Qwen3-30B-A3B-Instruct-2507** (served as an OpenAI-compatible API).

## 2) Install into VERL

1) Clone VERL and locate the reward directory:

```bash
git clone https://github.com/verl-project/verl.git
cd verl
ls verl/utils/reward_score
```

2) Copy the reward modules into VERL:

```bash
cp /path/to/askQ/reward/ask_mind_qa.py verl/utils/reward_score/ask_mind_qa.py
cp /path/to/askQ/reward/overconfidence_qa.py verl/utils/reward_score/overconfidence_qa.py
```

3) Register them in `verl/utils/reward_score/__init__.py` by adding the following branches inside
`def default_compute_score(...):`

```python
    if data_source == "ask_mind_qa":
        from .ask_mind_qa import compute_score_ask_mind_qa
        return compute_score_ask_mind_qa(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=extra_info,
        )

    if data_source == "overconfidence_qa":
        from .overconfidence_qa import compute_score_overconfidence_qa
        return compute_score_overconfidence_qa(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=extra_info,
        )
```

## 3) Required `extra_info` fields (high level)

Both modules implement **turn-level** rewards:

- Non-final turn: discrete shaping based on checklist coverage (aligned with the paper’s `{-2.0, -0.8, 0.8, 1.0}`).
- Final turn: judge correctness only (`still_asking / wrong / correct`).

AskMind expects (via `extra_info`) keys like:

- `is_final_turn`, `ori_question`, `degraded_info`, `required_points`, `question`, `context`, `expected_answer`

AskOverconfidence expects keys like:

- `is_final_turn`, `ori_question`, `overconfidence_info`, `misleading_points`, `question`, `context`, `expected_answer`

For code-level details and prompts, see `reward/readme_for_ai.md`.
