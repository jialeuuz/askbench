# data_pipeline (readme_for_ai)

This document is implementation-oriented (for maintainers / debugging / LLM-assisted edits). For a quick “how to run” guide, see `README.md`. The original Chinese version is preserved as `data_pipeline/readme_for_ai_zh.md`.

Mapping to the paper (`paper.pdf`):

- **AskMind (intent-deficient / missing information)**: `degraded_question` + `required_points` correspond to the rubric/checkpoints for missing information.
- **AskOverconfidence (misleading claims)**: `overconfidence_question` + `misleading_points` correspond to the rubric/checkpoints for injected misleading claims.
- **Judge loop (offline construction)**: the strategy-level “coverage self-check + judge decision + (optional) force-correction”, used to construct controlled multi-turn trajectories offline.

---

## Data pipeline overview

An asynchronous, strategy-based data construction pipeline that turns raw QA examples into multi-turn dialogue training data (or “direct answer + correction” data). Features:

- Streaming JSONL processing for large files (avoid holding everything in memory)
- Resume support: automatically skips examples already present in the success/failure outputs
- Batched, concurrent calls to a custom Chat Completions API
- Persistent failure records for reruns and prompt debugging

## Layout

- `main.py`: entry point (read input, call APIs in batches, resume, write outputs)
- `strategies.py`: core strategy implementations (multi-turn, direct-answer+correction, ...)
- `post_api.py`: API client (OpenAI Chat Completions compatible)
- `prompt_loader.py`: loads templates from `prompts.txt`
- `prompts.txt`: all prompt templates (names start with `template_`)
- `main_queue.py`, `run_queue.sh`: multi-job parallel scheduling (optional)
- `tools/`: small utilities for JSONL merge/sample/convert
- `utils/`: shared API base classes and URL health checks

## Setup

- Python 3.9+
- Install deps:
  - `pip install -r requirements.txt`
  - Core deps include `requests`, `aiohttp`, `tqdm`
- You need an OpenAI-compatible Chat Completions API that returns `choices[0].message.content`

## I/O schema

Input: JSONL (one example per line). Recommended minimum fields:

```json
{
  "id": "optional; deterministically hashed from content if missing",
  "ori_question": "original question",
  "expected_answer": "reference answer",
  "solution": "optional reasoning/solution; used as a reference for force-correction"
}
```

Output: JSONL (successful items). Fields vary by strategy. Common ones:

- `conversation_history`: multi-turn message list, e.g. `{ "role": "user|assistant", "content": "..." }`
- `degraded_question`, `degraded_info`, `required_points`: degraded question + missing-info checklist generated by strategies

Failures: JSONL file with the same name plus a `_failed` suffix. Each failure includes `_failure` metadata:

```json
"_failure": {
  "step": "failed step name",
  "reason": "failure reason / parse error",
  "attempts": 2,
  "response_preview": "optional; truncated model output"
}
```

## Running (single job)

Edit the constants at the bottom of `main.py`, or call `main()` from your own script.

Key parameters:

- `STRATEGY`: strategy name (see below)
- `INPUT_FILE`: input JSONL path
- `OUTPUT_FILE`: output JSONL path (failures go to `<output>_failed.jsonl`)
- `API_URLS`: list of Chat Completions endpoints (round-robin under concurrency)
- `API_TYPE`: model/type string passed to the backend `model` field
- `API_TOKEN`: optional auth token
- `PROMPTS_FILE`: prompt template file (default `prompts.txt`)
- `MAX_CONCURRENT_REQUESTS`: max concurrent requests
- `TIMEOUT`: per-request timeout (seconds)
- `BATCH_SIZE`: batch size (tune based on backend rate limits)
- `ID_KEY`: unique key field (default `id`)
- `REPROCESS_FAILED`: whether to rerun historical failures (default false); can also be set via env var `REPROCESS_FAILED=1`

Example (run the built-in config in `main.py`):

```bash
python main.py
```

Example (call from your own script):

```python
import asyncio
from main import main

asyncio.run(main(
    strategy="generate_multi_turn_degraded_training_data",
    input_file="/path/to/input.jsonl",
    output_file="/path/to/output.jsonl",
    prompts_file="prompts.txt",
    api_urls=["http://host:port/v1/chat/completions"],
    api_type="default",
    api_token="none",
    max_concurrent_requests=200,
    timeout=3600,
    batch_size=1000,
    id_key="id",
    reprocess_failed=False,
))
```

## Strategies (`strategies.py`)

All built-in strategies return a tuple: `(completed_items, failed_items)`.

1) `generate_degraded_question_and_info`
- Input: `ori_question`, `expected_answer`
- Output: `degraded_question`, `degraded_info`, `required_points`
- Often used as a prerequisite step for degraded multi-turn strategies

2) `generate_overconfidence_question_and_info`
- Input: `ori_question`, `expected_answer`
- Output:
  - `overconfidence_question`: a standalone, natural user query that preserves the original givens, but injects confidently-stated wrong intermediate claims in the explanation/derivation (without changing conditions or numbers)
  - `overconfidence_info`: records wrong assertions vs. correct facts (and their impact)
  - `misleading_points`: a checklist of misleading claims to be challenged/corrected
- Used as the prerequisite step for overconfidence multi-turn strategies; the gold answer still corresponds to the original question and is not affected by the injected misleading claims

3) `generate_multi_turn_degraded_training_data`
- Goal: generate a full multi-turn dialogue for degraded questions (ask → simulate user → coverage check → answer → judge → force-correct)
- `required_points` drives what must be clarified each turn; if present, `solution` is used as additional context for force-correction prompts

4) `generate_multi_turn_overconfidence_training_data`
- Goal: generate a multi-turn dialogue for “user is overconfident but the reasoning is wrong (without changing the problem conditions)”, by interrogating and correcting `misleading_points`
- Mirrors the `ask_eval` overconfidence setting: the user does not provide new information and does not “do the correction” (only accept/reject the assistant’s corrections). The assistant must proactively identify and correct misleading claims before giving the final answer

5) `strategy_direct_answer_and_correct`
- Goal: generate a direct answer first, judge it, and if wrong reconstruct a “perfect answer” based on `expected_answer` (and optional `solution`)
- Shorter flow; useful when multi-turn clarification is not needed

## Development notes (adding/modifying strategies)

- Convention: strategy functions are `async def xxx(api_client, data, templates, ...) -> (completed_items, failed_items)` and return “success list + failure list”.
- Prefer reusing `strategies._run_multi_turn_strategy(...)` to reduce duplication (Ask → SimUser → Coverage → Answer → Judge → ForceCorrect).
- When adding templates to `prompts.txt`, ensure the name starts with `template_` and the content is wrapped in triple quotes `'''...'''` so `prompt_loader.load_prompts()` can parse it.
- Template variable injection happens in `strategies._run_batch_step_with_retry()`:
  - If you add new variables, populate `format_args` there. `_safe_format()` only substitutes known keys to avoid `str.format` KeyErrors caused by JSON braces in templates.
- JSON schema parsing/validation:
  - Variant generation (degraded/overconfidence) uses `_parse_question_variant_json_response()` to extract and validate required keys.
  - Coverage checking uses `_parse_coverage_json_response()`, and judging uses `_parse_judge_json_response()`.
  - Parse failures are retried automatically; persistent failures are written to `_failed.jsonl` with `_failure.response_preview` for prompt debugging.

## Prompt templates

The multi-turn “user simulation / judge” prompts in this pipeline are aligned with `ask_eval`’s `AskEvaluator` **non-strict mode (`STRICT_MODE=0`)** to keep data construction consistent with evaluation:

- missing_info (degraded): `template_simulate_user_reply` / `template_judge_answer`
- overconfidence: `template_overconfidence_simulate_user_reply` / `template_overconfidence_judge_answer`

Alignment references: `ask_eval/ask_eval/evaluators/ask.py` (`SIMULATOR_PROMPT_TEMPLATE`, `OVERCONFIDENCE_SIMULATOR_PROMPT_TEMPLATE`, `ARBITER_EVALUATOR_PROMPT_TEMPLATE`, `ARBITER_EVALUATOR_PROMPT_TEMPLATE_OVERCONFIDENCE`).

### Injected template variables (in addition to example fields)

Besides per-example fields, `strategies._run_batch_step_with_retry()` injects convenience variables you can directly use in `prompts.txt`:

- `conversation_history_text`: `conversation_history` formatted as multi-line `role: content`
- `assistant_message`: last assistant message in the dialogue (used by overconfidence simulator)
- `assistant_question`: same as `assistant_message` but treated as a “question” (used by missing_info simulator)
- `scenario_question`: `overconfidence_question` → `degraded_question` → `ori_question` (first non-empty)
- `scenario_context`: `overconfidence_info` → `degraded_info` → `""` (first non-empty)
- `checklist_header`: checklist title (two variants: missing_info / overconfidence; aligned with `ask_eval`)
- `required_points_text`: checklist points formatted as a numbered list (with a fallback hint if empty)
- `user_internal_knowledge`: JSON string with `my_real_question`, `scenario_context`, `scenario_type`, `checklist_header`, `checklist_points`

Template format in `prompts.txt`:

```
template_xxx = '''
...your template content...
'''
```

Main template names (partial):

- `template_generate_degraded_question_and_info`
- `template_generate_overconfidence_question_and_info`
- `template_assistant_ask_first_question`
- `template_assistant_ask_follow_up_question`
- `template_assistant_ask_all_remaining`
- `template_simulate_user_reply`
- `template_coverage_check`
- `template_overconfidence_assistant_ask_first_question`
- `template_overconfidence_assistant_ask_follow_up_question`
- `template_overconfidence_assistant_ask_all_remaining`
- `template_overconfidence_simulate_user_reply`
- `template_overconfidence_coverage_check`
- `template_generate_final_answer`
- `template_generate_overconfidence_final_answer`
- `template_judge_answer`
- `template_overconfidence_judge_answer`
- `template_direct_answer`
- `template_judge_direct_answer`
- `template_reconstruct_answer`

## Parallel scheduling (optional)

- `run_queue.sh` + `main_queue.py` support configuring multiple “serial job queues” and running queues in parallel at the process level.
- Before use, fill in each job’s JSON (strategy, input/output, API_URLS, ...) in `run_queue.sh`.
- Note: `main.py` supports newer parameters such as `batch_size`, `id_key`, and `reprocess_failed`. If you need them in queued execution, extend `main_queue.py` to pass them through.

## FAQ

- The program exits immediately and produces no new outputs?
  - Likely all examples have already been processed into the success/failure files. Set `REPROCESS_FAILED=1` to rerun failures.
- Many parse failures?
  - Verify your backend returns a standard Chat Completions schema, or adjust the prompts using `_failure.response_preview` in the logs.
- 429 / rate limiting?
  - Lower `MAX_CONCURRENT_REQUESTS`, reduce `BATCH_SIZE`, or add more endpoints in `API_URLS`.

## Disclaimer

Make sure you are authorized to call the target API and comply with applicable terms of service and safety requirements.
