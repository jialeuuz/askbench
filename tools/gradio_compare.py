import gradio as gr
import requests
import json
from typing import Generator, List, Dict

def call_llm_api(api_url: str, messages: List[Dict], api_key: str = "EMPTY") -> Generator[str, None, None]:
    """
    è°ƒç”¨OpenAIæ ¼å¼çš„LLM APIï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    data = {
        "model": "default",
        "messages": messages,
        "stream": True,
        "temperature": 0.7
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=data, stream=True, timeout=60)
        response.raise_for_status()
        
        full_response = ""
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    line = line[6:]
                    if line.strip() == '[DONE]':
                        break
                    try:
                        json_data = json.loads(line)
                        if 'choices' in json_data and len(json_data['choices']) > 0:
                            delta = json_data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                full_response += content
                                yield full_response
                    except json.JSONDecodeError:
                        continue
        
        if not full_response:
            yield "âš ï¸ æœªæ”¶åˆ°æœ‰æ•ˆå“åº”"
            
    except requests.exceptions.RequestException as e:
        yield f"âŒ è¯·æ±‚é”™è¯¯: {str(e)}"
    except Exception as e:
        yield f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"

def call_llm_api_non_stream(api_url: str, messages: List[Dict], api_key: str = "EMPTY") -> str:
    """
    è°ƒç”¨OpenAIæ ¼å¼çš„LLM APIï¼ˆéæµå¼è¾“å‡ºï¼‰
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    data = {
        "model": "default",
        "messages": messages,
        "stream": False,
        "temperature": 0.7
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=data, timeout=60)
        response.raise_for_status()
        result = response.json()
        
        if 'choices' in result and len(result['choices']) > 0:
            return result['choices'][0]['message']['content']
        else:
            return "âš ï¸ æœªæ”¶åˆ°æœ‰æ•ˆå“åº”"
            
    except requests.exceptions.RequestException as e:
        return f"âŒ è¯·æ±‚é”™è¯¯: {str(e)}"
    except Exception as e:
        return f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"

def format_chat_history(history: List[List]) -> str:
    """
    æ ¼å¼åŒ–èŠå¤©å†å²æ˜¾ç¤º
    """
    if not history:
        return ""
    
    formatted = ""
    for i, (user_msg, bot_msg) in enumerate(history, 1):
        formatted += f"**[ç¬¬{i}è½®å¯¹è¯]**\n\n"
        formatted += f"ğŸ‘¤ **ç”¨æˆ·**: {user_msg}\n\n"
        if bot_msg:
            formatted += f"ğŸ¤– **åŠ©æ‰‹**: {bot_msg}\n\n"
        formatted += "---\n\n"
    return formatted

def build_messages(history: List[List], current_question: str) -> List[Dict]:
    """
    æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²å¯¹è¯ï¼‰
    """
    messages = []
    
    # æ·»åŠ å†å²å¯¹è¯
    for user_msg, assistant_msg in history:
        messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})
    
    # æ·»åŠ å½“å‰é—®é¢˜
    if current_question:
        messages.append({"role": "user", "content": current_question})
    
    return messages

def compare_models(question: str, api1_url: str, api2_url: str, use_stream: bool, 
                  history1: List[List], history2: List[List]):
    """
    åŒæ—¶è°ƒç”¨ä¸¤ä¸ªæ¨¡å‹APIå¹¶è¿”å›ç»“æœï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    """
    if not question.strip():
        return history1, history1, "", history2, history2, ""
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages1 = build_messages(history1, question)
    messages2 = build_messages(history2, question)
    
    if use_stream:
        # æµå¼è¾“å‡º
        gen1 = call_llm_api(api1_url, messages1)
        gen2 = call_llm_api(api2_url, messages2)
        
        response1 = ""
        response2 = ""
        done1, done2 = False, False
        
        # å…ˆæ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
        new_history1 = history1 + [[question, None]]
        new_history2 = history2 + [[question, None]]
        
        while not (done1 and done2):
            try:
                if not done1:
                    response1 = next(gen1)
            except StopIteration:
                done1 = True
            
            try:
                if not done2:
                    response2 = next(gen2)
            except StopIteration:
                done2 = True
            
            # æ›´æ–°å†å²è®°å½•
            temp_history1 = history1 + [[question, response1]]
            temp_history2 = history2 + [[question, response2]]
            
            yield (temp_history1, format_chat_history(temp_history1), "", 
                   temp_history2, format_chat_history(temp_history2), "")
        
        # æœ€ç»ˆç»“æœ
        final_history1 = history1 + [[question, response1]]
        final_history2 = history2 + [[question, response2]]
        
        return (final_history1, format_chat_history(final_history1), "",
                final_history2, format_chat_history(final_history2), "")
    else:
        # éæµå¼è¾“å‡º
        response1 = call_llm_api_non_stream(api1_url, messages1)
        response2 = call_llm_api_non_stream(api2_url, messages2)
        
        new_history1 = history1 + [[question, response1]]
        new_history2 = history2 + [[question, response2]]
        
        return (new_history1, format_chat_history(new_history1), "",
                new_history2, format_chat_history(new_history2), "")

def clear_history():
    """
    æ¸…ç©ºå¯¹è¯å†å²ï¼Œå¼€å§‹æ–°å¯¹è¯
    """
    return [], "", "", [], "", ""

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="LLMæ¨¡å‹å¯¹æ¯”å·¥å…·", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ¤– LLMæ¨¡å‹å¯¹æ¯”å·¥å…·
        åŒæ—¶æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹çš„å›å¤æ•ˆæœï¼Œæ”¯æŒå¤šè½®å¯¹è¯ï¼Œæ–¹ä¾¿å¯¹æ¯”åˆ†æ
        """
    )
    
    # çŠ¶æ€å˜é‡ï¼šå­˜å‚¨ä¸¤ä¸ªæ¨¡å‹çš„å¯¹è¯å†å²
    chat_history1 = gr.State([])
    chat_history2 = gr.State([])
    
    with gr.Row():
        with gr.Column():
            api1_input = gr.Textbox(
                label="æ¨¡å‹1 APIåœ°å€",
                value="http://10.80.13.48:8012/v1/chat/completions",
                placeholder="è¾“å…¥ç¬¬ä¸€ä¸ªAPIçš„å®Œæ•´URL"
            )
        with gr.Column():
            api2_input = gr.Textbox(
                label="æ¨¡å‹2 APIåœ°å€",
                value="http://10.80.13.48:8013/v1/chat/completions",
                placeholder="è¾“å…¥ç¬¬äºŒä¸ªAPIçš„å®Œæ•´URL"
            )
    
    with gr.Row():
        question_input = gr.Textbox(
            label="è¾“å…¥é—®é¢˜",
            placeholder="åœ¨è¿™é‡Œè¾“å…¥ä½ æƒ³é—®çš„é—®é¢˜...",
            lines=3,
            scale=4
        )
        with gr.Column(scale=1):
            stream_checkbox = gr.Checkbox(label="å¯ç”¨æµå¼è¾“å‡º", value=True)
            with gr.Row():
                submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", size="lg")
                clear_btn = gr.Button("ğŸ”„ æ–°å¯¹è¯", variant="secondary", size="lg")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ“ æ¨¡å‹1å¯¹è¯å†å²")
            output1 = gr.Markdown(label="", height=500)
        with gr.Column():
            gr.Markdown("### ğŸ“ æ¨¡å‹2å¯¹è¯å†å²")
            output2 = gr.Markdown(label="", height=500)
    
    # ç»‘å®šæäº¤äº‹ä»¶
    submit_btn.click(
        fn=compare_models,
        inputs=[question_input, api1_input, api2_input, stream_checkbox, 
                chat_history1, chat_history2],
        outputs=[chat_history1, output1, question_input, 
                 chat_history2, output2, question_input]
    )
    
    # æ”¯æŒå›è½¦æäº¤
    question_input.submit(
        fn=compare_models,
        inputs=[question_input, api1_input, api2_input, stream_checkbox,
                chat_history1, chat_history2],
        outputs=[chat_history1, output1, question_input,
                 chat_history2, output2, question_input]
    )
    
    # æ¸…ç©ºå†å²æŒ‰é’®
    clear_btn.click(
        fn=clear_history,
        inputs=[],
        outputs=[chat_history1, output1, question_input,
                 chat_history2, output2, question_input]
    )
    
    gr.Markdown(
        """
        ---
        ### ä½¿ç”¨è¯´æ˜
        1. **é…ç½®APIåœ°å€**: ç¡®è®¤ä¸¤ä¸ªæ¨¡å‹çš„APIåœ°å€æ˜¯å¦æ­£ç¡®
        2. **è¾“å…¥é—®é¢˜**: åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥ä½ æƒ³æµ‹è¯•çš„é—®é¢˜
        3. **æµå¼è¾“å‡º**: å¯é€‰æ‹©æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆå®æ—¶çœ‹åˆ°ç”Ÿæˆè¿‡ç¨‹ï¼‰
        4. **å‘é€é—®é¢˜**: ç‚¹å‡»"ğŸš€ å‘é€"æŒ‰é’®æˆ–æŒ‰å›è½¦é”®æäº¤
        5. **å¤šè½®å¯¹è¯**: ç»§ç»­è¾“å…¥é—®é¢˜å³å¯è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œæ¨¡å‹ä¼šè®°ä½ä¸Šä¸‹æ–‡
        6. **æ–°å¯¹è¯**: ç‚¹å‡»"ğŸ”„ æ–°å¯¹è¯"æŒ‰é’®æ¸…ç©ºå†å²ï¼Œå¼€å§‹æ–°çš„å¯¹è¯
        7. **å¯¹æ¯”åˆ†æ**: å·¦å³å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›å¤æ•ˆæœå’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
        
        ### åŠŸèƒ½ç‰¹ç‚¹
        - âœ… æ”¯æŒå¤šè½®å¯¹è¯ï¼Œè‡ªåŠ¨ä¿æŒä¸Šä¸‹æ–‡
        - âœ… åŒæ—¶å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„è¡¨ç°
        - âœ… æ”¯æŒæµå¼å’Œéæµå¼è¾“å‡º
        - âœ… å®Œæ•´çš„å¯¹è¯å†å²æ˜¾ç¤º
        """
    )

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7866,
        share=False
    )