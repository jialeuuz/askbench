import pandas as pd
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from asyncio import Semaphore
from datetime import datetime
from tqdm import tqdm
import requests
import asyncio
import aiohttp
import base64
import json
import time
from typing import List, Union
import threading
import traceback

# å¼‚æ­¥é™æµå™¨ (ç”¨äº asyncio æ¨¡å¼)
class RateLimiter:
    def __init__(self, rate):
        self._interval = 1.0 / rate
        self._last_time = None

    async def wait(self):
        now = time.monotonic()
        if self._last_time is None:
            self._last_time = now
            return
        elapsed = now - self._last_time
        if elapsed < self._interval:
            await asyncio.sleep(self._interval - elapsed)
        self._last_time = time.monotonic()

# åŒæ­¥é™æµå™¨ (ç”¨äºå¤šçº¿ç¨‹æ¨¡å¼)
class SyncRateLimiter:
    def __init__(self, rate):
        self._interval = 1.0 / rate
        self._last_time = None
        self._lock = threading.Lock() # ç¡®ä¿åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„åŸå­æ€§æ“ä½œ

    def wait(self):
        with self._lock:
            now = time.monotonic()
            if self._last_time is None:
                self._last_time = now
                return
            
            elapsed = now - self._last_time
            sleep_for = self._interval - elapsed
            if sleep_for > 0:
                time.sleep(sleep_for)
            self._last_time = time.monotonic()

class ZnyConfig:
    def __init__(self, url: Union[str, List[str]], model_name: str = 'gpt4o', temperature: float = 0.9, max_retries: int = 5, retry_until_success: bool = False, 
                 qps: int = 2, # æ­¤å¤„çš„QPSç°åœ¨ä»£è¡¨ *æ¯ä¸ªAPIç«¯ç‚¹* çš„ä¸Šé™
                 max_concurrent: int = 10, chunk_size: int = 1, asyncio_flag: bool = False, image_flag: bool = False, image_column_name: str = None, input_column_name: str = 'input', response_column_name: str = "assistant",
                 top_p=0.95, repetition_penalty=1, max_tokens=6400, 
                 resume_from_output: bool = False,
                 save_interval: int = 100):
        
        if isinstance(url, str):
            self.urls = [u.strip() for u in url.split(',') if u.strip()]
        elif isinstance(url, list):
            self.urls = [u.strip() for u in url if u.strip()]
        else:
            raise ValueError("url must be a string (comma-separated) or a list of strings.")
        
        if not self.urls:
            raise ValueError("No valid URLs provided.")

        self.model_name = model_name
        self.chunk_size = chunk_size
        self.max_retries = max_retries
        self.retry_until_success = retry_until_success
        self.temperature = temperature
        self.qps = qps
        self.max_concurrent = max_concurrent
        self.asyncio_flag = asyncio_flag
        self.image_flag = image_flag
        self.image_column_name = image_column_name
        self.input_column_name = input_column_name
        self.response_column_name = response_column_name
        self.top_p = top_p
        self.repetition_penalty = repetition_penalty
        self.max_tokens = max_tokens
        self.resume_from_output = resume_from_output
        self.save_interval = save_interval

class CallLLMByZny(object):
    def __init__(self, config: ZnyConfig):
        self.config = config
        self.url_list = self.config.urls
        
        self._url_index = 0
        self._url_lock = threading.Lock()
        
        self.rate_limiters = {}
        if config.asyncio_flag:
            for url in self.url_list:
                self.rate_limiters[url] = RateLimiter(config.qps)
        else:
            for url in self.url_list:
                self.rate_limiters[url] = SyncRateLimiter(config.qps)

        print(f"ğŸš€ LLMæœåŠ¡å·²åˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨ä»¥ä¸‹APIç«¯ç‚¹è¿›è¡Œè´Ÿè½½å‡è¡¡: {self.url_list}")
        print(f"âš¡ï¸ é™æµç­–ç•¥: æ¯ä¸ªAPIç«¯ç‚¹çš„QPSä¸Šé™ä¸º {config.qps} (æ€»å¹¶å‘è¿æ¥æ•°ä¸Šé™ä¸º {config.max_concurrent})")

    def _get_next_url(self) -> str:
        """çº¿ç¨‹å®‰å…¨åœ°è·å–ä¸‹ä¸€ä¸ªURLï¼Œå®ç°è½®è¯¢ã€‚"""
        with self._url_lock:
            url = self.url_list[self._url_index]
            self._url_index = (self._url_index + 1) % len(self.url_list)
            return url

    def _save_progress(self, df_to_append: pd.DataFrame, out_path: str):
        if df_to_append.empty:
            return
        try:
            jsonl_string = df_to_append.to_json(
                orient="records", lines=True, force_ascii=False
            )
            if not jsonl_string.endswith("\n"):
                jsonl_string += "\n"
            
            with open(out_path, 'a', encoding='utf-8') as f:
                f.write(jsonl_string)
            print(f"âœ” è¿›åº¦å·²ä¿å­˜ï¼šæˆåŠŸè¿½åŠ  {len(df_to_append)} æ¡è®°å½•åˆ° {out_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜è¿›åº¦æ—¶å‡ºé”™: {e}")

    def _read_jsonl_robustly(self, file_path: str) -> pd.DataFrame:
        valid_records = []
        corrupted_lines_count = 0
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        valid_records.append(json.loads(line))
                    except json.JSONDecodeError:
                        corrupted_lines_count += 1
                        print(f"  - è­¦å‘Š: åœ¨æ–‡ä»¶ {os.path.basename(file_path)} ä¸­æ£€æµ‹åˆ°å¹¶è·³è¿‡ç¬¬ {i+1} è¡Œçš„æŸåæ•°æ®ã€‚")
            
            if corrupted_lines_count > 0:
                print(f"  - æ€»è®¡: ä» {os.path.basename(file_path)} ä¸­æˆåŠŸåŠ è½½ {len(valid_records)} æ¡è®°å½•ï¼Œå¿½ç•¥äº† {corrupted_lines_count} æ¡æŸåçš„è®°å½•ã€‚")

            if not valid_records:
                return pd.DataFrame()
            
            return pd.DataFrame(valid_records)
        except FileNotFoundError:
            return pd.DataFrame()
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {file_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return pd.DataFrame()

    def get_gpt4api_df(self, init_prompt_df: pd.DataFrame, out_path: str) -> pd.DataFrame:
        prompt_df = init_prompt_df.copy()
        out_name = os.path.basename(out_path)
        
        processed_inputs = set()
        if self.config.resume_from_output and os.path.exists(out_path):
            print(f"æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ {out_path}ï¼Œæ­£åœ¨å°è¯•æ–­ç‚¹é‡ç»­...")
            processed_df = self._read_jsonl_robustly(out_path)
            
            if not processed_df.empty:
                if self.config.input_column_name in processed_df.columns:
                    processed_inputs = set(processed_df[self.config.input_column_name])
                    print(f"å·²ä» {out_path} åŠ è½½ {len(processed_inputs)} æ¡æœ‰æ•ˆè®°å½•ã€‚")
                else:
                    print(f"è­¦å‘Šï¼šè¾“å‡ºæ–‡ä»¶ä¸­ç¼ºå°‘è¾“å…¥åˆ— '{self.config.input_column_name}'ï¼Œæ— æ³•è¿›è¡Œç²¾ç¡®çš„æ–­ç‚¹é‡ç»­ã€‚å°†é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®ã€‚")
            else:
                print("è¾“å‡ºæ–‡ä»¶ä¸ºç©ºæˆ–æ‰€æœ‰è¡Œå‡å·²æŸåã€‚å°†ä»å¤´å¼€å§‹å¤„ç†ã€‚")

        df_to_process = prompt_df[~prompt_df[self.config.input_column_name].isin(processed_inputs)].copy()
        
        if df_to_process.empty:
            print("æ‰€æœ‰æ•°æ®å‡å·²å¤„ç†å®Œæ¯•ã€‚")
            return self._read_jsonl_robustly(out_path) if os.path.exists(out_path) else pd.DataFrame()

        print(f"æ€»è®¡ {len(prompt_df)} æ¡ï¼Œå·²å¤„ç† {len(processed_inputs)} æ¡ï¼Œæœ¬æ¬¡éœ€å¤„ç† {len(df_to_process)} æ¡ã€‚")
        df_to_process.reset_index(drop=True, inplace=True)
        
        all_prompts_ls = list(zip(
            df_to_process.index,
            df_to_process[self.config.input_column_name].to_list(),
            df_to_process[self.config.image_column_name].to_list() if self.config.image_column_name and self.config.image_column_name in df_to_process.columns else [None] * len(df_to_process)
        ))

        if self.config.asyncio_flag:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            loop.run_until_complete(self._process_and_save_stream(all_prompts_ls, df_to_process, out_path, out_name))
        else:
            self._process_and_save_stream_threaded(all_prompts_ls, df_to_process, out_path, out_name)

        print(f"æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆã€‚æ­£åœ¨è¯»å–æœ€ç»ˆç»“æœæ–‡ä»¶...")
        if os.path.exists(out_path):
            return self._read_jsonl_robustly(out_path)
        else:
            return pd.DataFrame()

    def _process_and_save_stream_threaded(self, indexed_prompts, source_df, out_path, out_name):
        newly_completed_rows = []
        with tqdm(total=len(indexed_prompts), desc=f"{out_name}è¿›åº¦") as pbar:
            with ThreadPoolExecutor(max_workers=self.config.max_concurrent) as executor:
                futures = {executor.submit(self._process_one_prompt_with_index, index, [prompt], image_path): index 
                           for index, prompt, image_path in indexed_prompts}

                for future in as_completed(futures):
                    try:
                        original_index, response_data = future.result()
                        response_content = self._parser_one_response(response_data)
                        
                        if "<|ERROR" in response_content or "<|PARSING_ERROR|>" in response_content:
                            print(f"  -> ä»»åŠ¡ {original_index} å¤±è´¥ï¼Œå·²è·³è¿‡ä¿å­˜ã€‚é”™è¯¯: {response_content}")
                            continue

                        row_data = source_df.loc[original_index].to_dict()
                        row_data[self.config.response_column_name] = response_content
                        newly_completed_rows.append(row_data)
                        
                        if self.config.save_interval > 0 and len(newly_completed_rows) >= self.config.save_interval:
                            self._save_progress(pd.DataFrame(newly_completed_rows), out_path)
                            newly_completed_rows = []

                    except Exception as e:
                        print(f"å¤„ç†ä»»åŠ¡ {original_index} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {repr(e)}")
                        traceback.print_exc()
                    finally:
                        pbar.update(1)
        
        if newly_completed_rows:
            self._save_progress(pd.DataFrame(newly_completed_rows), out_path)

    async def _process_prompt_with_retries(self, semaphore, session, index, prompt, image_path, max_retries=5):
        for attempt in range(1, max_retries + 1):
            try:
                # æ‰§è¡Œä¸€æ¬¡å®é™…è°ƒç”¨
                original_index, response_data = await self._process_one_prompt_async_with_index(
                    semaphore, session, index, [prompt], image_path
                )
                response_content = self._parser_one_response(response_data)

                # åˆ¤æ–­è¿”å›å€¼æ˜¯å¦åŒ…å«é”™è¯¯æ ‡è®°
                if "<|ERROR" in response_content or "<|PARSING_ERROR|>" in response_content:
                    print(f"ä»»åŠ¡ {original_index} ç¬¬ {attempt} æ¬¡è¿”å›é”™è¯¯ï¼Œå‡†å¤‡é‡è¯•...")
                    await asyncio.sleep(1)  # å¯é€‰: ç­‰å¾…1sé¿å…è¿ç¯é”™è¯¯
                    continue

                # æˆåŠŸ
                return True, original_index, response_content

            except Exception as e:
                print(f"å¤„ç†ä»»åŠ¡ {index} ç¬¬ {attempt} æ¬¡å‘ç”Ÿå¼‚å¸¸: {repr(e)}")
                traceback.print_exc()
                await asyncio.sleep(1)  # é¿å…é¢‘ç¹é‡è¯•

        # å¦‚æœåˆ°äº†è¿™é‡Œï¼Œè¡¨ç¤º5æ¬¡éƒ½å¤±è´¥
        return False, index, None

    async def _process_and_save_stream(self, indexed_prompts, source_df, out_path, out_name):
        semaphore = Semaphore(self.config.max_concurrent)
        newly_completed_rows = []

        async with aiohttp.ClientSession() as session:
            tasks = [
                self._process_prompt_with_retries(
                    semaphore, session, index, prompt, image_path, max_retries=5
                )
                for index, prompt, image_path in indexed_prompts
            ]

            with tqdm(total=len(tasks), desc=f"{out_name}è¿›åº¦") as pbar:
                for future in asyncio.as_completed(tasks):
                    success, original_index, response_content = await future

                    if success:
                        row_data = source_df.loc[original_index].to_dict()
                        row_data[self.config.response_column_name] = response_content
                        newly_completed_rows.append(row_data)

                        if self.config.save_interval > 0 and len(newly_completed_rows) >= self.config.save_interval:
                            self._save_progress(pd.DataFrame(newly_completed_rows), out_path)
                            newly_completed_rows = []
                    else:
                        print(f"ä»»åŠ¡ {original_index} é‡è¯• {5} æ¬¡ä¾æ—§å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")

                    pbar.update(1)

        if newly_completed_rows:
            self._save_progress(pd.DataFrame(newly_completed_rows), out_path)

    def _process_one_prompt_with_index(self, index: int, prompt: list, image_path: str = None) -> tuple:
        response = self._request_one_chat(prompt, image_path)
        return index, response

    async def _process_one_prompt_async_with_index(self, semaphore, session, index: int, prompts: list, image_path: str) -> tuple:
        response = await self._request_one_chat_async(semaphore, session, prompts, image_path)
        return index, response

    def _request_one_chat(self, messages: list, image_path: str):
        headers = {'Content-Type': 'application/json'}
        # <<< æ”¹åŠ¨ 2.1: ä¸º "default" æ¨¡å‹è‡ªåŠ¨æ·»åŠ è®¤è¯å¤´
        if self.config.model_name == 'default':
            headers['Authorization'] = 'Bearer EMPTY'
            
        data_entry = self._make_chat_request_entry(messages, image_path)
        retries = 0
        last_exception = None

        while True:
            url = self._get_next_url()
            limiter = self.rate_limiters[url]
            
            limiter.wait()
            try:
                response = requests.post(url, headers=headers, json=data_entry, timeout=60)
                response.raise_for_status()
                return response.json()
            except requests.exceptions.RequestException as e:
                last_exception = e
                print(f"  - è¯·æ±‚URLå¤±è´¥: {url}. é”™è¯¯: {e}. å°†åœ¨ä¸‹ä¸€æ¬¡å°è¯•ä¸­ä½¿ç”¨ä¸‹ä¸€ä¸ªURLã€‚")

            retries += 1
            if not self.config.retry_until_success and retries >= self.config.max_retries:
                error_msg = f"å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°({self.config.max_retries})ï¼Œæ‰€æœ‰å°è¯•å‡å¤±è´¥ã€‚æœ€ç»ˆé”™è¯¯: {last_exception}"
                print(f"âŒ {error_msg}")
                return {"error": error_msg}
            
            retry_msg = f"æ— é™é‡è¯•... (å°è¯•æ¬¡æ•°: {retries})" if self.config.retry_until_success else f"é‡è¯•... (å°è¯• {retries}/{self.config.max_retries})"
            print(f"  - {retry_msg}")
            time.sleep(2)

    async def _request_one_chat_async(self, semaphore, session, messages, image_path):
        headers = {'Content-Type': 'application/json'}
        if self.config.model_name == 'default':
            headers['Authorization'] = 'Bearer EMPTY'

        data_entry = self._make_chat_request_entry(messages, image_path)
        retries = 0
        last_exception = None

        # è¿™é‡Œç”¨ aiohttp.ClientTimeout å¯ä»¥ç»†åˆ†è¿æ¥ã€è¯»å–ã€æ€»è¶…æ—¶
        timeout_cfg = aiohttp.ClientTimeout(
            total=120,     # æ•´ä¸ªè¯·æ±‚æœ€é•¿æ—¶é—´
            connect=10,    # TCP è¿æ¥é˜¶æ®µè¶…æ—¶
            sock_read=800  # ç­‰å¾…æœåŠ¡å™¨å“åº”æ•°æ®çš„æœ€é•¿æ—¶é—´
        )

        while True:
            url = self._get_next_url()
            limiter = self.rate_limiters[url]

            try:
                async with semaphore:
                    await limiter.wait()
                    async with session.post(url, json=data_entry, headers=headers, timeout=timeout_cfg) as response:
                        response.raise_for_status()
                        return await response.json()

            except asyncio.TimeoutError as e:
                # â³ æ˜ç¡®æ‰“å°è¶…æ—¶ï¼Œå¹¶è¿›å…¥é‡è¯•é€»è¾‘
                print(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼š{url} (ç¬¬ {retries+1} æ¬¡å°è¯•ï¼Œå…± {self.config.max_retries} æ¬¡)")
                # traceback.print_exc()
                last_exception = e

            except aiohttp.ClientError as e:
                # aiohttp è¿æ¥ç±»é”™è¯¯ï¼ˆConnectionResetError, ServerDisconnectedErrorç­‰ï¼‰
                print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼š{url} é”™è¯¯ç±»å‹: {type(e).__name__}, ä¿¡æ¯: {e}")
                # traceback.print_exc()
                last_exception = e

            except Exception as e:
                # å…¶ä»–æœªçŸ¥é”™è¯¯
                print(f"âŒ æœªçŸ¥å¼‚å¸¸ï¼š{url} é”™è¯¯ç±»å‹: {type(e).__name__}, ä¿¡æ¯: {e}")
                traceback.print_exc()
                last_exception = e

            # =========================
            # ç»Ÿä¸€çš„é‡è¯•åˆ¤å®šä¸ç­‰å¾…é€»è¾‘
            # =========================
            retries += 1
            if not self.config.retry_until_success and retries >= self.config.max_retries:
                error_msg = f"å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°({self.config.max_retries})ï¼Œæœ€åé”™è¯¯ç±»å‹: {type(last_exception).__name__}, ä¿¡æ¯: {last_exception}"
                print(f"âŒ {error_msg}")
                return {"error": error_msg}

            retry_msg = (
                f"æ— é™é‡è¯•ï¼ˆå½“å‰å·²å°è¯• {retries} æ¬¡ï¼‰..."
                if self.config.retry_until_success
                else f"é‡è¯•ä¸­... (å°è¯• {retries}/{self.config.max_retries})"
            )
            print(f"  - {retry_msg}")

            # å¯ä»¥æ”¹æˆæŒ‡æ•°é€€é¿ï¼ˆexponential backoffï¼‰é™ä½å‹åŠ›
            await asyncio.sleep(min(2 * retries, 10))

    def encode_image(self, image_path: str):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def _make_chat_request_entry(self, messages: list, image_path) -> dict:
        prompt_text = messages[0]

        if self.config.model_name == "gpt4o":
            if self.config.image_flag:
                assert image_path is not None, "image_flag = True è€Œ image path ä¸ºç©ºï¼Œæ— æ³•è§£ç "
                try:
                    base64_image = self.encode_image(image_path)
                except Exception as e:
                    raise IOError(f"æ— æ³•è¯»å–æˆ–ç¼–ç å›¾ç‰‡: {image_path}") from e
                data_entry = {
                    "messages": [{"role": "user", "contents": [{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}, {"type": "text", "text": prompt_text}]}],
                    "temperature": self.config.temperature
                }
            else:
                data_entry = {
                    "messages": [{'role' : 'user', 'contents' : [{"type": "text","text": prompt_text}]}],
                    "temperature": self.config.temperature
                }
        elif self.config.model_name in ["gpt4", "wenxin"] or "claude" in self.config.model_name:
            data_entry = {
                "messages": [{"role": "user", "content": msg} for msg in messages],
                "temperature": self.config.temperature
            }
        elif self.config.model_name == 'default':
            data_entry = {
                "model": "default",
                "messages": [{"role": "user", "content": msg} for msg in messages],
                "temperature": self.config.temperature,
                "max_tokens": self.config.max_tokens
            }
        elif "deepseek" in self.config.model_name:
            data_entry = {
                "messages": [{"role": "user", "content": msg} for msg in messages],
                "temperature": self.config.temperature,
                "top_p": self.config.top_p,
                "repetition_penalty": self.config.repetition_penalty,
                "max_tokens": self.config.max_tokens
            }
        elif any(m in self.config.model_name for m in ["o1-mini", "o4-mini", "gpt_41", "gemini_2_5_pro", "gpt_5"]):
            return {
                "messages": [{"role": "user", "content": [{"type": "text", "text": prompt_text}]}],
            }
        else:
            data_entry = {
                "messages": [{"role": "user", "content": msg} for msg in messages],
                "temperature": self.config.temperature
            }

        return data_entry

    def _parser_one_response(self, response_item: dict):
        try:
            if 'error' in response_item:
                return f"<|ERROR: {response_item['error']}|>"
            
            if self.config.model_name == 'gpt4o':
                return response_item['data']['choices'][0]['content']
            elif self.config.model_name == 'wenxin':
                return response_item['data']['result']
            # <<< æ”¹åŠ¨ 4: å°† "default" æ·»åŠ åˆ°æ­¤è§£æé€»è¾‘ä¸­
            elif self.config.model_name in ['claude', 'deepseek_v3', 'gpt_5', 'default'] or any(m in self.config.model_name for m in ["o1-mini", "o4-mini", "gpt_41", "gemini_2_5_pro"]):
                return response_item['choices'][0]['message']['content']
            elif self.config.model_name == 'deepseek_r1':
                assistant1 = response_item['choices'][0]['message']['content']
                assistant_reasoning = response_item['choices'][0]['message']['reasoning_content']
                return json.dumps({'response': assistant1, 'reasoning': assistant_reasoning}, ensure_ascii=False)
            else:
                return response_item['choices'][0]['message']['content']
        except (KeyError, IndexError, TypeError) as e:
            return f"<|PARSING_ERROR: {e} - Response: {str(response_item)[:200]}|>"

def read_data(file: Union[str, list, dict]):
    def read_one_file(file_path: str, rate: float = 1.0) -> pd.DataFrame:
        print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path} ...")
        if file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
        elif file_path.endswith('.jsonl'):
            records = []
            corrupted_lines_count = 0
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            records.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            corrupted_lines_count += 1
                            print(f"âš ï¸ è­¦å‘Š: åœ¨è¾“å…¥æ–‡ä»¶ {os.path.basename(file_path)} çš„ç¬¬ {i+1} è¡Œè§£æJSONæ—¶å‡ºé”™ã€‚å·²è·³è¿‡æ­¤è¡Œã€‚")
                            print(f"   é”™è¯¯ä¿¡æ¯: {e}")
                            print(f"   é—®é¢˜è¡Œå†…å®¹ (å‰150å­—ç¬¦): {line[:150]}")
                
                if corrupted_lines_count > 0:
                    print(f"â„¹ï¸ æ€»è®¡: ä» {os.path.basename(file_path)} æˆåŠŸåŠ è½½ {len(records)} æ¡è®°å½•ï¼Œå¿½ç•¥äº† {corrupted_lines_count} æ¡æŸåçš„è®°å½•ã€‚")

                if not records:
                    df = pd.DataFrame()
                else:
                    df = pd.DataFrame(records)

            except FileNotFoundError:
                print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ° {file_path}")
                return pd.DataFrame()
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶ {file_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                return pd.DataFrame()

        elif file_path.endswith('.json'):
            df = pd.read_json(file_path, orient="records")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
            
        if not df.empty:
            if rate > 1:
                n_samples = int(rate * len(df))
                df = df.sample(n=n_samples, replace=True, random_state=42).reset_index(drop=True)
            elif 0 < rate < 1:
                df = df.sample(frac=rate, random_state=42).reset_index(drop=True)
        
        print(f"# {os.path.basename(file_path)}: åŸå§‹ {len(records) if 'records' in locals() else len(df)} æ¡, åŠ è½½å¹¶é‡‡æ ·åæ•°æ®é‡ä¸º {len(df)}")
        return df

    if isinstance(file, str):
        df = read_one_file(file)
    elif isinstance(file, list):
        df = pd.concat([read_one_file(path) for path in file], ignore_index=True)
        print(f"åˆå¹¶åæ€»æ•°æ®é‡: {len(df)}")
    elif isinstance(file, dict):
        assert 'rate' in file and 'path' in file, 'å­—å…¸æ ¼å¼å¿…é¡»åŒ…å« `rate` å’Œ `path` é”®'
        df = pd.concat([read_one_file(path, rate=file['rate'][i]) for i, path in enumerate(file['path'])], ignore_index=True)
        print(f"åˆå¹¶åæ€»æ•°æ®é‡: {len(df)}")
    else:
        raise TypeError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(file)}")
    
    return df

def fill_prompt_by_key_mappings(df, template: str, key_mappings: dict, prompt_key: str = "prompt") -> pd.DataFrame:
    # ... æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œä¿æŒåŸæ · ...
    filled_inputs = []
    for _, row in df.iterrows():
        filled_template = template
        for placeholder, real_key in key_mappings.items():
            filler = str(row.get(real_key, ''))
            filled_template = filled_template.replace(f'{{{placeholder}}}', filler)
        filled_inputs.append(filled_template)

    df[prompt_key] = filled_inputs
    return df

# <<< æ”¹åŠ¨ 1: åœ¨å­—å…¸ä¸­æ·»åŠ æ‚¨çš„API
ZNY_API_URLS = {
    "gpt4o": "https://yangshuling-gpt4o.fc.chj.cloud/gpt4o/chat",
    "claude": "https://yangshuling-claude.fc.chj.cloud/claude35_sonnet/conversation",
    "o1-mini": "https://yangshuling-deepseek.fc.chj.cloud/o1-mini",
    "o4-mini": "https://yangshuling-deepseek.fc.chj.cloud/o4-mini",
    "claude-37": "https://yangshuling-deepseek.fc.chj.cloud/claude-37",
    "deepseek_r1": "https://yangshuling-deepseek.fc.chj.cloud/deepseek_r1",
    "gpt_41": "https://gpt41.fc.chj.cloud/gpt_41,https://yangjingwen.fc.chj.cloud/gpt_41,https://jiale-de-deepseek.fc.chj.cloud/gpt_41",
    "gpt_5": "https://yangjingwen.fc.chj.cloud/gpt_5, https://jiale-de-deepseek.fc.chj.cloud/gpt_5,https://linzhiyu-gemini.fc.chj.cloud/gpt_5",
    "claude_opus_41": "https://jiale-de-deepseek.fc.chj.cloud/claude_opus_41",
    "gemini_2_5_pro": "https://linzhiyu-gemini.fc.chj.cloud/gemini_2_5_pro",
    "default": "http://10.80.12.172:8012/v1/chat/completions, http://10.80.12.172:8013/v1/chat/completions" # æ–°å¢æ‚¨çš„æ¨¡å‹
}

if __name__ == "__main__":
    # ================== 1. é…ç½®åŒºåŸŸ ==================
    model_name = 'default'
    input_file = "/lpai/volumes/base-ov-ali-sh-mix/zhaojiale/askQ/data/train_data/single_turn/sample_20k_gpt_oss_120b.jsonl"
    out_file = "/lpai/volumes/base-ov-ali-sh-mix/zhaojiale/askQ/data/gpt_res/sample_20k_gpt_oss_120b_2turn.jsonl"

    config = ZnyConfig(
        # URLä¼šè‡ªåŠ¨ä» ZNY_API_URLS å­—å…¸ä¸­è·å–
        url=ZNY_API_URLS[model_name],
        model_name=model_name,
        temperature=0.7,
        max_tokens=16000,
        max_retries=10,
        retry_until_success=False, 
        qps=100, # æ ¹æ®æ‚¨çš„APIæœåŠ¡å™¨æ‰¿å—èƒ½åŠ›è°ƒæ•´
        max_concurrent=100, # æ ¹æ®æ‚¨çš„APIæœåŠ¡å™¨æ‰¿å—èƒ½åŠ›è°ƒæ•´
        asyncio_flag=True,
        image_flag=False,
        image_column_name=None,
        input_column_name='prompt',
        response_column_name='gpt_res_2', # å¯ä»¥è‡ªå®šä¹‰è¾“å‡ºåˆ—å
        resume_from_output=True,
        save_interval=50,
    )
    
    # å®Œæ•´çš„ template å†…å®¹ (ä¿æŒä¸å˜)
    # template = '''{query}'''
    template = '''You are an expert in generating conversational data. Your task is to create a two-turn dialogue based on an ambiguous initial question. The goal is to simulate a scenario where a helpful AI, instead of guessing, asks for clarification, and the user then provides the necessary information to get a correct answer.

**You must strictly follow these steps:**

1.  **Analyze the Input:**
    - You will be given an ambiguous/incomplete question (`degraded_question`).
    - You will also be given a detailed explanation of why it is ambiguous (`degraded_info`). This explanation tells you exactly what critical information was removed and what terms were made vague.
    - You will be given the correct final answer to the *original*, non-degraded question (`answer`).

2.  **Step 1: Generate the AI's Clarifying Question (the `ask` field).**
    - Act as a helpful but cautious AI assistant.
    - Read the `degraded_question` and use the `degraded_info` to identify the specific points of ambiguity or missing information.
    - Formulate a polite, natural-sounding question that asks the user to provide the exact information needed to resolve the ambiguity.
    - **Do not attempt to answer the question.** Your only goal is to seek clarification. For example, ask "Could you please specify what you mean by 'a certain pattern'?" or "To give you the most accurate answer, could you tell me the specific medical term you're referring to?".

3.  **Step 2: Generate the User's Follow-up and the AI's Final Answer (the `question_2` and `answer_2` fields).**
    - **a. Formulate the User's Clarifying Response (`question_2`):**
        - Now, switch roles and act as the user.
        - Your response should directly and concisely answer the AI's clarifying question from the previous step.
        - Use the `degraded_info` to find the *original, precise information* that was removed or obfuscated. This is what the user provides. For example: "Oh, sorry. I meant 'Onion-skin fibrosis'." or "Yes, the specific condition I'm asking about is Primary Biliary Cirrhosis."
    - **b. Formulate the AI's Final, Correct Answer (`answer_2`):**
        - Switch back to the role of the AI assistant.
        - Now that you have the complete, unambiguous information (from the `degraded_question` + `question_2`), provide the final, correct answer.
        - This answer **must** match the provided `answer` field.

4.  **Output Format:**
    - Return the result **only** in the following JSON structure.
    - **Do not include any explanations, comments, or markdown formatting outside of this JSON.**

    ```json
    {
        "ask": "<The AI's clarifying question from Step 2>",
        "question_2": "<The user's response providing the missing information from Step 3a>",
        "answer_2": "<The AI's final, correct answer from Step 3b>"
    }
    ```

**Here is the data to process:**

**Ambiguous Question**
{degraded_question}

**Degradation Info**
{degraded_info}

**Correct Final Answer**
{answer}
'''

    placeholder_mappings = {
        "text": "llm_prompt",
        "expected_answer": "answer",
        "query": "query",
        "degraded_question": "degraded_question",
        "degraded_info": "degraded_info",
        "answer": "answer",
    }
    
    # ================== 2. æ‰§è¡Œæµç¨‹ (ä¿æŒä¸å˜) ==================
    print("--- ä»»åŠ¡å¼€å§‹ ---")
    
    out_dir = os.path.dirname(out_file)
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
        print(f"å·²åˆ›å»ºç›®å½•: {out_dir}")

    print("\n[æ­¥éª¤ 1/4] æ­£åœ¨è¯»å–è¾“å…¥æ•°æ®...")
    df = read_data(input_file)
    if df.empty:
        print("è¾“å…¥æ•°æ®ä¸ºç©ºæˆ–æ— æ³•è¯»å–ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        exit()
    
    print("\n[æ­¥éª¤ 2/4] æ­£åœ¨æ ¹æ®æ¨¡æ¿å¡«å……Prompt...")
    prompt_df = fill_prompt_by_key_mappings(df, template, placeholder_mappings, prompt_key=config.input_column_name)
    print(f"å·²ä¸º {len(prompt_df)} æ¡è®°å½•ç”ŸæˆPromptã€‚")
    
    print("\n[æ­¥éª¤ 3/4] æ­£åœ¨åˆå§‹åŒ–å¹¶è°ƒç”¨LLM API...")
    call_zny = CallLLMByZny(config)
    final_df = call_zny.get_gpt4api_df(prompt_df, out_file)
    
    print("\n[æ­¥éª¤ 4/4] æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ ¡éªŒå’Œä¿å­˜...")
    initial_count = len(prompt_df)
    final_count = len(final_df) if final_df is not None else 0

    if initial_count != final_count:
        print("\n" + "="*60)
        print("âš ï¸  è­¦å‘Š: æ•°æ®é‡ä¸åŒ¹é…ï¼ âš ï¸")
        print(f"    - åŸå§‹è¾“å…¥æ•°æ®é‡: {initial_count}")
        print(f"    - æœ€ç»ˆè¾“å‡ºæ•°æ®é‡: {final_count}")
        print(f"    - ä»æœ‰ {initial_count - final_count} æ¡è®°å½•æœªæˆåŠŸå¤„ç†ã€‚")
        print("    - è¯·æ£€æŸ¥æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯ï¼Œæˆ–é‡æ–°è¿è¡Œæ­¤è„šæœ¬ä»¥å¤„ç†å‰©ä½™çš„ä»»åŠ¡ã€‚")
        print("="*60 + "\n")
    else:
        print(f"\nâœ… æ ¡éªŒé€šè¿‡ï¼šæ‰€æœ‰ {initial_count} æ¡è®°å½•å‡å·²æˆåŠŸå¤„ç†å¹¶ä¿å­˜ã€‚\n")

    if final_df is not None and not final_df.empty:
        json_out_path = out_file.replace('.jsonl', '.json')
        try:
            final_df.to_json(json_out_path, indent=2, force_ascii=False, orient='records')
            print(f"å·²å°†æœ€ç»ˆç»“æœè½¬æ¢ä¸ºæ ‡å‡†JSONæ ¼å¼: {json_out_path}")
        except Exception as e:
            print(f"è½¬æ¢ä¸ºæ ‡å‡†JSONæ—¶å‡ºé”™: {e}")
            
    print("--- ä»»åŠ¡å®Œæˆ ---")
