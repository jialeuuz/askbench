[path]
# Directory to save evaluation outputs
save_dir = results/example-run


[model]
# model_type: backend type (currently only "api" is supported)
# api_type: model identifier on your OpenAI-compatible server (e.g., "gpt-4o-mini")
# sk_token: bearer token / API key (use "none" if not required)
# api_url: OpenAI-compatible endpoint base URL (we auto-normalize to /chat/completions when needed)
# timeout: request timeout in seconds
# system_prompt: optional system prompt
# extra_prompt: optional suffix appended to each question

model_type = api
api_type = default
sk_token = none
api_url = http://host:port/v1
timeout = 600
model_name = default

[generateconfig]
top_p = 0.8
top_k = 20
max_tokens = 16000
temperature = 0.7
max_concurrent = 100

[tasks]
# Comma-separated task list
# Available tasks:
# math500, medqa, gpqa, bbh,
# ask_mind, ask_overconfidence,
# ask_overconfidence_math500, ask_overconfidence_medqa, ask_overconfidence_gpqa, ask_overconfidence_bbh,
# quest_bench,
# ask_mind_math500de, ask_mind_medqade, ask_mind_gpqade, ask_mind_bbhde,
# fata_math500, fata_medqa,
# healthbench
enabled = math500,medqa
# Directory containing per-task INI configs
tasks_config_path = config/common

[evaluatorconfig]
model_type = api
api_type = default
api_url = http://host:port/v1
timeout = 600
max_concurrent = 100
temperature = 0.0
max_new_tokens = 2048
max_turns = 3
sk_token = none
model_name = default
